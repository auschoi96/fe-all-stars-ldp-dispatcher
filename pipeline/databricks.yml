bundle:
  name: windmill-iot-streaming

sync:
  include:
    - "src/**/*.py"
    - "src/**/*.ipynb"
    - "src/**/*.txt"
    - "src/**/*.yml"
    - "src/**/*.html"
    - "src/**/*.css"
    - "src/**/*.js"
    - "databricks.yml"

workspace:
  host: https://e2-demo-field-eng.cloud.databricks.com
  profile: e2-demo-field-eng
  
variables:
  catalog:
    description: Unity Catalog to use (defaults to user's short name)
    default: users
  schema:
    description: Schema to use for tables
    default: "${workspace.current_user.short_name}"

resources:
  jobs:
    windmill_data_generator:
      name: "Windmill IoT Data Generator - ${workspace.current_user.short_name}"
      description: "Generates synthetic windmill IoT data using dbldatagen"
      job_clusters:
        - job_cluster_key: "main_cluster"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 0  # Single-node cluster
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
            custom_tags:
              ResourceClass: "SingleNode"
            # Enable Unity Catalog
            data_security_mode: "SINGLE_USER"
            runtime_engine: "PHOTON"
      tasks:
        - task_key: "generate_windmill_data"
          job_cluster_key: "main_cluster"
          spark_python_task:
            python_file: "./src/windmill_data_generator/main.py"
            parameters:
              - "--catalog"
              - "${var.catalog}"
              - "--schema"
              - "${var.schema}"
              - "--table"
              - "raw_sensor_data"
              - "--records-per-batch"
              - "10"
              - "--batch-interval"
              - "15"
              - "--max-iterations"
              - "100000"
          libraries:
            - pypi:
                package: "dbldatagen>=0.3.0"
      max_concurrent_runs: 1
      timeout_seconds: 3600

    agent_monitoring:
      name: "Agent Monitor - ${workspace.current_user.short_name}"
      description: "Continuous Agent to review Turbine data and submit tickets"
      continuous:
        pause_status: "UNPAUSED"
      tasks:
        - task_key: "agent_monitoring_job"
          notebook_task:
            notebook_path: "./src/Agent_interaction.ipynb"
            base_parameters:
              instance_name: "fe_shared_demo"
              current_username: "${workspace.current_user.userName}"
              current_shortname: "${workspace.current_user.short_name}"
              batch_interval: "180"
          environment_key: serverless
      environments:
        - environment_key: serverless
          spec:
            client: "1"
            dependencies:
              - dspy==3.0.3
              - mlflow==3.4.0
              - databricks-sdk==0.67.0
      max_concurrent_runs: 1
      timeout_seconds: 36000

  pipelines:
    windmill_lakeflow_pipeline:
      name: "Windmill IoT Lakeflow Pipeline - ${workspace.current_user.short_name}"
      catalog: "${var.catalog}"
      target: "${var.schema}"
      libraries:
        - file:
            path: "./src/lakeflow_pipeline.py"
      configuration:
        "pipelines.applyChangesPreviewTableOptions": "true"
        # Enable event log publishing for incremental refresh monitoring
        "pipelines.enableEventLogging": "true"
      photon: true
      serverless: true
      continuous: true
#
#  apps:
#    fe_all_stars_dashboard:
#      name: "windmill-iot-streaming"
#      description: "FE All Stars Agent Dispatcher Dashboard"
#      source_code_path: "./src/app"